name: Data Warehouse CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      run_type:
        description: 'Type of run to execute'
        required: true
        default: 'validation'
        type: choice
        options:
        - validation
        - full_pipeline
        - config_check

env:
  PYTHON_VERSION: '3.9'
  DATABASE_SERVER: ${{ secrets.DATABASE_SERVER || 'localhost' }}
  DATABASE_NAME: 'DataWarehouse'

jobs:
  # ============================================================================
  # Code Quality and Testing
  # ============================================================================
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black mypy pytest pytest-cov
        pip install -r requirements.txt
    
    - name: Run Black code formatter check
      run: |
        black --check --diff pipelines/ monitoring/
    
    - name: Run Flake8 linter
      run: |
        flake8 pipelines/ monitoring/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 pipelines/ monitoring/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Run MyPy type checker
      run: |
        mypy pipelines/ --ignore-missing-imports
      continue-on-error: true
    
    - name: Run Python syntax check
      run: |
        python -m py_compile pipelines/*.py
        python -m py_compile pipelines/config/*.py
        python -m py_compile pipelines/utils/*.py
        python -m py_compile monitoring/*.py

  # ============================================================================
  # SQL Validation
  # ============================================================================
  sql-validation:
    name: SQL Script Validation
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Install SQL validation tools
      run: |
        sudo apt-get update
        sudo apt-get install -y sqlite3
    
    - name: Validate SQL scripts syntax
      run: |
        # Basic SQL syntax validation using sqlite3
        echo "Validating SQL scripts..."
        
        # Check for common SQL syntax issues
        find scripts/ -name "*.sql" -exec echo "Checking {}" \;
        
        # Validate that all SQL files contain valid T-SQL keywords
        if grep -r "EXEC\|SELECT\|INSERT\|UPDATE\|DELETE\|CREATE\|DROP\|ALTER" scripts/; then
          echo "✓ SQL keywords found in scripts"
        else
          echo "✗ No SQL keywords found"
          exit 1
        fi
        
        # Check for balanced parentheses and brackets
        find scripts/ -name "*.sql" -exec sh -c '
          file="$1"
          if ! python3 -c "
import sys
content = open(\"$file\").read()
if content.count(\"(\") != content.count(\")\"): 
    print(f\"Unbalanced parentheses in $file\")
    sys.exit(1)
if content.count(\"[\") != content.count(\"]\"): 
    print(f\"Unbalanced brackets in $file\")
    sys.exit(1)
print(f\"✓ $file syntax OK\")
          "
          then echo "✓ $file validated"
          else echo "✗ $file validation failed"; exit 1
          fi
        ' _ {} \;

  # ============================================================================
  # Configuration Validation
  # ============================================================================
  config-validation:
    name: Configuration Validation
    runs-on: ubuntu-latest
    needs: [code-quality]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pyyaml
        pip install -r requirements.txt
    
    - name: Validate YAML configuration
      run: |
        python -c "
import yaml
import sys
try:
    with open('pipelines/config/config.yaml', 'r') as f:
        config = yaml.safe_load(f)
    print('✓ Configuration YAML is valid')
    
    # Check required sections
    required_sections = ['database', 'logging', 'pipeline', 'data_sources']
    for section in required_sections:
        if section not in config:
            print(f'✗ Missing required section: {section}')
            sys.exit(1)
        else:
            print(f'✓ Found section: {section}')
    
    print('✓ All required configuration sections present')
except Exception as e:
    print(f'✗ Configuration validation failed: {e}')
    sys.exit(1)
        "
    
    - name: Test configuration loading
      run: |
        cd pipelines
        python -c "
from config.config import config_manager
config = config_manager.get_database_config()
print(f'✓ Database config loaded: {config.server}/{config.database}')
"

  # ============================================================================
  # Data Validation Tests
  # ============================================================================
  data-validation:
    name: Data Validation Tests
    runs-on: ubuntu-latest
    needs: [config-validation, sql-validation]
    if: github.event.inputs.run_type != 'config_check'
    
    services:
      sqlserver:
        image: mcr.microsoft.com/mssql/server:2022-latest
        env:
          SA_PASSWORD: 'YourStrong!Passw0rd'
          ACCEPT_EULA: 'Y'
        ports:
          - 1433:1433
        options: >-
          --health-cmd "/opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P 'YourStrong!Passw0rd' -Q 'SELECT 1'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
        # Install SQL Server ODBC driver
        curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -
        curl https://packages.microsoft.com/config/ubuntu/20.04/prod.list | sudo tee /etc/apt/sources.list.d/msprod.list
        sudo apt-get update
        sudo ACCEPT_EULA=Y apt-get install -y msodbcsql18 unixodbc-dev
    
    - name: Wait for SQL Server to be ready
      run: |
        echo "Waiting for SQL Server to be ready..."
        for i in {1..30}; do
          if /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P 'YourStrong!Passw0rd' -Q "SELECT 1" > /dev/null 2>&1; then
            echo "SQL Server is ready!"
            break
          fi
          echo "Waiting for SQL Server... ($i/30)"
          sleep 2
        done
    
    - name: Initialize test database
      run: |
        # Create test database and schemas
        /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P 'YourStrong!Passw0rd' -Q "
        CREATE DATABASE DataWarehouse;
        USE DataWarehouse;
        CREATE SCHEMA bronze;
        CREATE SCHEMA silver;  
        CREATE SCHEMA gold;
        "
    
    - name: Create test configuration
      run: |
        mkdir -p pipelines/config
        cat > pipelines/config/test_config.yaml << EOF
        database:
          server: "localhost"
          database: "DataWarehouse"
          username: "sa"
          password: "YourStrong!Passw0rd"
          trusted_connection: false
          driver: "ODBC Driver 18 for SQL Server"
        
        logging:
          level: "INFO"
          file_path: "test_logs/pipeline.log"
        
        pipeline:
          batch_size: 1000
          max_retries: 2
          timeout: 60
        
        data_sources:
          crm_path: "datasets/source_crm/"
          erp_path: "datasets/source_erp/"
        
        data_quality:
          enable_validation: true
        EOF
    
    - name: Run configuration test
      run: |
        cd pipelines
        export CONFIG_PATH="config/test_config.yaml"
        python -c "
from config.database import DatabaseManager
from config.config import ConfigManager
import os

config_path = os.environ.get('CONFIG_PATH', 'config/config.yaml')
config_manager = ConfigManager(config_path)
db_manager = DatabaseManager(config_manager.get_database_config())

if db_manager.test_connection():
    print('✓ Database connection test passed')
else:
    print('✗ Database connection test failed')
    exit(1)
        "

  # ============================================================================
  # Security and Compliance Checks
  # ============================================================================
  security-scan:
    name: Security and Compliance
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Run secret detection
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        base: main
        head: HEAD
        extra_args: --debug --only-verified
    
    - name: Check for hardcoded credentials
      run: |
        echo "Checking for hardcoded credentials..."
        
        # Check for common credential patterns
        if grep -r -i "password\s*=" --include="*.py" --include="*.yaml" --include="*.sql" . | grep -v "password.*input\|password.*prompt\|password.*ask"; then
          echo "⚠️ Potential hardcoded passwords found"
        else
          echo "✓ No hardcoded passwords detected"
        fi
        
        # Check for SQL injection patterns
        if grep -r "SELECT.*+.*" --include="*.py" .; then
          echo "⚠️ Potential SQL injection vulnerability found"
        else  
          echo "✓ No obvious SQL injection patterns detected"
        fi
    
    - name: Check file permissions
      run: |
        echo "Checking file permissions..."
        find . -type f -name "*.sql" -o -name "*.py" | xargs ls -la
        
        # Ensure no executable permissions on data files
        if find datasets/ -type f -executable 2>/dev/null | grep -q .; then
          echo "⚠️ Executable data files found"
        else
          echo "✓ No executable data files"
        fi

  # ============================================================================
  # Generate Documentation
  # ============================================================================
  documentation:
    name: Generate Documentation
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install documentation tools
      run: |
        python -m pip install --upgrade pip
        pip install sphinx sphinx-rtd-theme
    
    - name: Generate API documentation
      run: |
        echo "Generating API documentation..."
        mkdir -p docs/api
        
        # Create simple API documentation
        cat > docs/api/README.md << 'EOF'
        # API Documentation
        
        ## Pipeline Modules
        
        ### ETL Pipeline (`etl_pipeline.py`)
        Main orchestrator for the ETL pipeline process.
        
        ### Data Loader (`data_loader.py`)  
        Handles loading data from various sources.
        
        ### Data Validator (`data_validator.py`)
        Provides data quality validation capabilities.
        
        ### Configuration (`config/`)
        Configuration management for database connections and pipeline settings.
        
        ### Monitoring (`monitoring/`)
        Metrics collection and dashboard functionality.
        EOF
    
    - name: Generate database schema documentation
      run: |
        echo "Generating database schema documentation..."
        mkdir -p docs/schema
        
        cat > docs/schema/README.md << 'EOF'
        # Database Schema Documentation
        
        ## Bronze Layer
        Raw data ingestion layer - stores data as-is from source systems.
        
        ## Silver Layer  
        Cleansed and standardized data layer - applies business rules and data quality checks.
        
        ## Gold Layer
        Analytics-ready data layer - implements star schema for optimal query performance.
        
        ### Star Schema Design
        - **Fact Table**: `fact_sales` - Contains sales transactions and metrics
        - **Dimension Tables**: 
          - `dim_customers` - Customer master data
          - `dim_products` - Product catalog information
        EOF
    
    - name: Commit documentation
      if: github.ref == 'refs/heads/main'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add docs/
        git diff --staged --quiet || git commit -m "Auto-update documentation [skip ci]"
        git push

  # ============================================================================
  # Deployment and Release
  # ============================================================================
  deploy:
    name: Deploy Pipeline
    runs-on: ubuntu-latest
    needs: [code-quality, sql-validation, config-validation, data-validation, security-scan]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Create deployment package
      run: |
        echo "Creating deployment package..."
        mkdir -p deployment
        
        # Copy essential files
        cp -r pipelines/ deployment/
        cp -r scripts/ deployment/
        cp -r monitoring/ deployment/
        cp requirements.txt deployment/
        cp README.md deployment/
        
        # Create deployment instructions
        cat > deployment/DEPLOY.md << 'EOF'
        # Deployment Instructions
        
        ## Prerequisites
        1. SQL Server instance available
        2. Python 3.9+ installed
        3. ODBC drivers installed
        
        ## Setup Steps
        1. Install Python dependencies: `pip install -r requirements.txt`
        2. Update configuration in `pipelines/config/config.yaml`
        3. Initialize database: Run scripts in `scripts/` directory
        4. Test connection: `python pipelines/run_etl_pipeline.py --config-check`
        5. Run pipeline: `python pipelines/run_etl_pipeline.py --full`
        
        ## Monitoring
        - View data quality dashboard: `python monitoring/dashboard.py`
        - Check logs in `logs/` directory
        EOF
        
        # Create version info
        echo "VERSION=$(date +%Y.%m.%d.%H%M)" >> deployment/VERSION
        echo "BUILD_NUMBER=${{ github.run_number }}" >> deployment/VERSION
        echo "COMMIT_SHA=${{ github.sha }}" >> deployment/VERSION
    
    - name: Upload deployment artifacts
      uses: actions/upload-artifact@v4
      with:
        name: data-warehouse-deployment-${{ github.run_number }}
        path: deployment/
        retention-days: 30
    
    - name: Create Release
      if: startsWith(github.ref, 'refs/tags/')
      uses: actions/create-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: ${{ github.ref_name }}
        release_name: Data Warehouse Release ${{ github.ref_name }}
        body: |
          ## Modern Data Warehouse Release
          
          ### Features
          - Complete ETL pipeline with Bronze-Silver-Gold architecture
          - Data quality monitoring and validation
          - Automated metrics collection and reporting
          - Web-based monitoring dashboard
          
          ### Installation
          1. Download the deployment artifact
          2. Follow instructions in DEPLOY.md
          3. Configure database connection
          4. Run the pipeline
          
          ### Changes
          See commit history for detailed changes.
        draft: false
        prerelease: false

  # ============================================================================
  # Notification and Reporting
  # ============================================================================
  notify:
    name: Send Notifications
    runs-on: ubuntu-latest
    needs: [deploy]
    if: always()
    
    steps:
    - name: Prepare notification message
      run: |
        if [[ "${{ needs.deploy.result }}" == "success" ]]; then
          echo "STATUS=✅ Success" >> $GITHUB_ENV
          echo "MESSAGE=Data Warehouse pipeline deployed successfully" >> $GITHUB_ENV
        elif [[ "${{ needs.deploy.result }}" == "failure" ]]; then
          echo "STATUS=❌ Failed" >> $GITHUB_ENV  
          echo "MESSAGE=Data Warehouse pipeline deployment failed" >> $GITHUB_ENV
        else
          echo "STATUS=⚠️ Skipped" >> $GITHUB_ENV
          echo "MESSAGE=Data Warehouse pipeline deployment skipped" >> $GITHUB_ENV
        fi
    
    - name: Post to Slack (if configured)
      if: env.SLACK_WEBHOOK_URL != ''
      run: |
        curl -X POST -H 'Content-type: application/json' \
          --data '{"text":"${{ env.STATUS }} ${{ env.MESSAGE }}\nBranch: ${{ github.ref_name }}\nCommit: ${{ github.sha }}\nRun: ${{ github.run_number }}"}' \
          ${{ secrets.SLACK_WEBHOOK_URL }}
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
    
    - name: Create summary
      run: |
        echo "## 🏢 Data Warehouse Pipeline Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Status:** ${{ env.STATUS }}" >> $GITHUB_STEP_SUMMARY
        echo "**Message:** ${{ env.MESSAGE }}" >> $GITHUB_STEP_SUMMARY
        echo "**Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "**Commit:** ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
        echo "**Run Number:** ${{ github.run_number }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Job Results" >> $GITHUB_STEP_SUMMARY
        echo "- Code Quality: ${{ needs.code-quality.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- SQL Validation: ${{ needs.sql-validation.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Config Validation: ${{ needs.config-validation.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Data Validation: ${{ needs.data-validation.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Security Scan: ${{ needs.security-scan.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Deployment: ${{ needs.deploy.result }}" >> $GITHUB_STEP_SUMMARY